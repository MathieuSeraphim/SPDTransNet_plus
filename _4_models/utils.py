import datetime
import warnings
from os import mkdir, getenv
from os.path import dirname, realpath, join, isdir
from jsonargparse import ArgumentParser
from typing import Any, Union, Dict, List
import torch
from torch import Tensor
from torch.nn import Module
from torch.nn import functional as F


def combine_dict_outputs(outputs: List[Dict]):
    keys_list = list(outputs[0].keys())
    combined_outputs_dict = {}
    for key in keys_list:
        combined_outputs_dict[key] = torch.cat([tmp[key] for tmp in outputs])
    return combined_outputs_dict


supported_loss_functions = ["cross_entropy"
                            "cross_entropy_with_label_smoothing"]
def get_loss_function_from_dict(loss_function_config_dict: Dict[str, Any], **kwargs):
    loss_function_name = loss_function_config_dict["name"]
    assert loss_function_name in supported_loss_functions
    if loss_function_name == "cross_entropy":
        return F.cross_entropy
    if loss_function_name == "cross_entropy_with_label_smoothing":
        if "args" in loss_function_config_dict.keys():
            extra_args = loss_function_config_dict["args"]
            return define_cross_entropy_loss_with_label_smoothing(**extra_args)
        return define_cross_entropy_loss_with_label_smoothing()
    raise NotImplementedError


def define_cross_entropy_loss_with_label_smoothing(label_smoothing: float = 0.1):
    def cross_entropy_loss_with_label_smoothing(input: torch.Tensor, target: torch.Tensor):
        return F.cross_entropy(input, target, label_smoothing=label_smoothing)
    return cross_entropy_loss_with_label_smoothing


def check_block_import(block: Union[Module, Dict]):
    if isinstance(block, Module):
        return block

    class ModuleWrapper:
        def __init__(self, module: Module):
            self.module = module

    assert isinstance(block, dict)
    parser = ArgumentParser()
    wrapper_dict = {"wrapper": {"module": block}}
    parser.add_class_arguments(ModuleWrapper, "wrapper", fail_untyped=False)
    return parser.instantiate_classes(wrapper_dict).wrapper.module


def get_timestamp_string():
    now = datetime.datetime.now()
    return now.strftime("%Y-%m-%dT%H:%M:%S") + ("-%02d" % (now.microsecond / 10000))


def save_problematic_tensor(tensor: Tensor, problem: str):
    current_script_directory = dirname(realpath(__file__))
    root_directory = dirname(current_script_directory)
    error_tensors_folder = join(root_directory, "error_tensors")
    if not isdir(error_tensors_folder):
        mkdir(error_tensors_folder)

    timestamp_string = get_timestamp_string()

    job_id_string = ""
    try:
        current_job_id = int(getenv("SLURM_ARRAY_TASK_ID"))
    except:
        current_job_id = None
    if current_job_id is not None:
        job_id_string = "job_%04d_" % current_job_id

    filename = "%s%s_%s.pt" % (job_id_string, timestamp_string, problem)
    full_filename = join(error_tensors_folder, filename)

    torch.save(tensor, open(full_filename, "wb"))


def error_message_function_string(function: Any, class_if_any: Any = None):
    function_name = function.__name__
    if class_if_any is not None:
        class_name = class_if_any.__name__
        function_string = "method %s of class %s" % (function_name, class_name)
    else:
        function_string = "function %s" % function_name
    return function_string


def save_error_tensor(tensor: Tensor, error: Exception, error_reason: str, function: Any, class_if_any: Any = None):
    error_name = error.__class__.__name__
    function_substring = error_message_function_string(function, class_if_any)
    timestamp_string = get_timestamp_string()
    warnings.warn("%s - %s generated by %s in %s." % (timestamp_string, error_name, error_reason, function_substring))
    save_problematic_tensor(tensor, error_name)


def non_finite_values_check(tensor_to_check: Tensor, function: Any, class_if_any: Any = None):
    if not torch.isfinite(tensor_to_check).all():
        error = ValueError("Tensor has non-finite values.")
        save_error_tensor(tensor_to_check, error, "non-finite values in Tensor", function, class_if_any)
        raise error


# Very costly to use!
def negative_eigenvalues_check(supposedly_spd_matrices: Tensor):
    if (torch.linalg.eigvals(supposedly_spd_matrices).real < 0).any():
        # save_problematic_tensor(supposedly_spd_matrices, "negative_eigenvalues")
        timestamp_string = get_timestamp_string()
        warnings.warn("%s - Matrix passing through SVD has negative eigenvalues!" % timestamp_string)

